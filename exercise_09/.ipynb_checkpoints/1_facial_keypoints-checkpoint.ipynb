{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Facial Keypoint Detection\n",
    " \n",
    "In this notebook, you will build a convolutional neural networks to perform facial keypoint detection.\n",
    "\n",
    "Before we start, let's take a look at some example images and corresponding facial keypoints:\n",
    "\n",
    "<img src='key_pts_example.png' width=50% height=50%/>\n",
    "\n",
    "The facial keypoints (also called facial landmarks) are the small magenta dots shown on each of the faces in the image above. These keypoints mark important areas of the face: the eyes, corners of the mouth, the nose, etc., and are relevant for a variety of computer vision tasks, such as face filters, emotion recognition, pose recognition, and more. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparation\n",
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "from exercise_code.data.facial_keypoints_dataset import FacialKeypointsDataset\n",
    "from exercise_code.networks.keypoint_nn import (\n",
    "    DummyKeypointModel,\n",
    "    KeypointModel\n",
    ")\n",
    "from exercise_code.util import (\n",
    "    show_all_keypoints,\n",
    "    save_model,\n",
    ")\n",
    "from exercise_code.tests import test_keypoint_nn\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# for auto-reloading external modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Visualize Data\n",
    "To load the data, we have already prepared a Pytorch Dataset class `FacialKeypointsDataset` for you. You can find it in `exercise_code/data/facial_keypoints/dataset.py`. Run the following cell to download the data and initialize your dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_url = 'http://filecremers3.informatik.tu-muenchen.de/~dl4cv/training.zip'\n",
    "i2dl_exercises_path = os.path.dirname(os.path.abspath(os.getcwd()))\n",
    "data_root = os.path.join(i2dl_exercises_path, \"datasets\", \"facial_keypoints\")\n",
    "train_dataset = FacialKeypointsDataset(\n",
    "    train=True,\n",
    "    transform=transforms.ToTensor(),\n",
    "    root=data_root,\n",
    "    download_url=download_url\n",
    ")\n",
    "val_dataset = FacialKeypointsDataset(\n",
    "    train=False,\n",
    "    transform=transforms.ToTensor(),\n",
    "    root=data_root,\n",
    ")\n",
    "print(\"Number of training samples:\", len(train_dataset))\n",
    "print(\"Number of validation samples:\", len(val_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each sample in our dataset is a dict `{\"image\": image, \"keypoints\": keypoints}`, where\n",
    " * `image` is a [0-1]-normalized gray-scale image of size 96x96, represented by a torch tensor of shape (CxHxW) with C=1, H=96, W=96\n",
    "    <img style=\"float: right;\" src='key_pts_expl.png' width=50% height=50%/>\n",
    " * `keypoints` is the list of K facial keypoints, stored in a torch tensor of shape (Kx2). We have K=15 keypoints that stand for:\n",
    "   * keypoints[0]: Center of the left eye\n",
    "   * keypoints[1]: Center of the right eye\n",
    "   * keypoints[2]: Left eye inner corner\n",
    "   * keypoints[3]: Left eye outer corner\n",
    "   * keypoints[4]: Right eye inner corner\n",
    "   * keypoints[5]: Right eye outer corner\n",
    "   * keypoints[6]: Left eyebrow inner end\n",
    "   * keypoints[7]: Left eyebrow outer end\n",
    "   * keypoints[8]: Right eyebrow inner end\n",
    "   * keypoints[9]: Right eyebrow outer end\n",
    "   * keypoints[10]: Nose tip\n",
    "   * keypoints[11]: Mouth left corner\n",
    "   * keypoints[12]: Mouth right corner\n",
    "   * keypoints[13]: Mouth center top lip\n",
    "   * keypoints[14]: Mouth center bottom lip\n",
    "\n",
    "   \n",
    "Each individual facial keypoint is represented by two coordinates (x,y) that specify the horizontal and vertical location of the keypoint respectively. All keypoint values are normalized to [-1,1], such that:\n",
    "   * (x=-1,y=-1) corresponds to the top left corner, \n",
    "   * (x=-1,y=1) to the bottom left corner,\n",
    "   * (x=1,y=-1) to the top right corner,\n",
    "   * (x=1,y=1) to the bottom right corner,\n",
    "   * and (x=0,y=0) to the center of the image.\n",
    "\n",
    "Let's have a look at the first training sample to get a better feeling for the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image, keypoints = train_dataset[0][\"image\"], train_dataset[0][\"keypoints\"]\n",
    "print(\"Shape of the image:\", image.size())\n",
    "print(\"Smallest value in the image:\", torch.min(image))\n",
    "print(\"Largest value in the image:\", torch.max(image))\n",
    "print(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keypoints = train_dataset[0][\"keypoints\"]\n",
    "print(keypoints)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In `exercise_code/util/vis_utils.py` we also provide you with a function `show_all_keypoints()` that takes in an image and keypoints and displays where the predicted keypoints are in the image. Let's use it to plot the first few images of our training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_keypoints(dataset, num_samples=3):\n",
    "    for i in range(num_samples):\n",
    "        image = dataset[i][\"image\"]\n",
    "        key_pts = dataset[i][\"keypoints\"]\n",
    "        show_all_keypoints(image, key_pts)\n",
    "show_keypoints(train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Facial Keypoint Detection\n",
    "Your task is to define and train a model for facial keypoint detection.\n",
    "\n",
    "The facial keypoint detection task can be seen as a regression problem, where the goal is to predict 30 different values that correspond to the 15 facial keypoint locations. Thus, we need to build a network that gets a (1x96x96) image as input and predicts 30 continuous outputs between [-1,1].\n",
    "\n",
    "## Dummy Model\n",
    "In `exercise_code/networks/dummy_network.py` we defined a naive `DummyModel`, which always predicts the keypoints of the first training image. Let's try it on a few images and visualize our predictions in red:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_keypoint_predictions(model, dataset, num_samples=3):\n",
    "    for i in range(num_samples):\n",
    "        image = dataset[i][\"image\"]\n",
    "        key_pts = dataset[i][\"keypoints\"]\n",
    "        predicted_keypoints = torch.squeeze(model(image).detach()).view(15,2)\n",
    "        show_all_keypoints(image, key_pts, predicted_keypoints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_model = DummyKeypointModel()\n",
    "show_keypoint_predictions(dummy_model, train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we see, the model predicts the first sample perfectly, but for the remaining samples the predictions are quite off.\n",
    "\n",
    "## Loss and Metrics\n",
    "\n",
    "To measure the quality of the model's predictions, we will use the mean squared error (https://en.wikipedia.org/wiki/Mean_squared_error), summed up over all 30 keypoint locations. In PyTorch, the mean squared error is defined in `torch.nn.MSELoss()`, and we can use it like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = torch.nn.MSELoss()\n",
    "for i in range(3):\n",
    "    image = train_dataset[i][\"image\"]\n",
    "    keypoints = train_dataset[i][\"keypoints\"]\n",
    "    predicted_keypoints = torch.squeeze(dummy_model(image)).view(15,2)\n",
    "    loss = loss_fn(keypoints, predicted_keypoints)\n",
    "    print(\"Loss on image %d:\" % i, loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, our dummy model achieves a loss close to 0 on the first sample, but on all other samples the loss is quite high."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To obtain an evaluation score (in the notebook and on the submission server), we will use the following function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, dataset):\n",
    "    model.eval()\n",
    "    criterion = torch.nn.MSELoss()\n",
    "    dataloader = DataLoader(dataset, batch_size=1, shuffle=False)\n",
    "    loss = 0\n",
    "    for batch in dataloader:\n",
    "        image, keypoints = batch[\"image\"], batch[\"keypoints\"]\n",
    "        predicted_keypoints = model(image).view(-1,15,2)\n",
    "        loss += criterion(\n",
    "            torch.squeeze(keypoints),\n",
    "            torch.squeeze(predicted_keypoints)\n",
    "        ).item()\n",
    "    return 1.0 / (2 * (loss/len(dataloader)))\n",
    "\n",
    "print(\"Score:\", evaluate_model(dummy_model, val_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**To pass the assignment, you will need to achieve a score of at least 100**. As you can see, the score is calculated from the average loss, so **your average loss needs to be lower than 0.005**. Our dummy model only gets a score of around 60, so you will have to come up with a better model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Design your own model\n",
    "Now it is your turn to build your own model. To do so, you need to design a convolutional neural network that takes images of size (Nx1x96x96) as input and produces outputs of shape (Nx30) in the range [-1,1].\n",
    "\n",
    "Recall that CNN's are defined by a few types of layers:\n",
    "* Convolutional layers\n",
    "* Maxpooling layers\n",
    "* Fully-connected layers\n",
    "\n",
    "You can design your network however you want, but we strongly suggest to include multiple convolutional layers. You are also encouraged to use things like dropout and batch normalization to stabilize and regularize your network. If you want to build a really competitive model, have a look at some literature on keypoint detection, such as [this paper](https://arxiv.org/pdf/1710.00977.pdf).\n",
    "\n",
    "#### Define your model in the provided file `exercise_code/classifiers/keypoint_nn.py` file\n",
    "\n",
    "This file is mostly empty but contains the expected class name, and the methods that your model needs to implement (only `forward()` basically). \n",
    "The only rules your model design has to follow are:\n",
    "* Inherit from torch.nn.Module or pytorch_lightning.LightningModule\n",
    "* Perform the forward pass in forward(), predicting keypoints of shape (Nx30) for images of shape (Nx1x96x96)\n",
    "* Have less than 5 million parameters\n",
    "* Have a model size of less than 20MB after saving\n",
    "\n",
    "Furthermore, you need to pass all your hyperparameters to the model in a single dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams = {\n",
    "    # TODO: if you have any model arguments/hparams, define them here\n",
    "}  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test whether your model follows the basic rules, run the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KeypointModel(hparams)\n",
    "test_keypoint_nn(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Train your model\n",
    "In addition to the network itself, you will also need to write the code for the model training. You can use PyTorch Lightning for that, or you can also write it yourself in standard PyTorch.\n",
    "\n",
    "**Hints:**\n",
    "* Use torch.nn.MSELoss() as loss function\n",
    "* Have a look at the previous notebooks for PyTorch and PyTorch Lightning in exercises 7 and 8 if you feel lost. In particular, revise 4_Cifar10_PytorchLightning.ipynb in exercise 8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################\n",
    "# TODO - Train Your Model                                              #\n",
    "########################################################################\n",
    "\n",
    "\n",
    "pass\n",
    "\n",
    "########################################################################\n",
    "#                           END OF YOUR CODE                           #\n",
    "########################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you're done training, run the cells below to visualize some predictions of your model, and to compute a validation score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_keypoint_predictions(model, val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Score:\", evaluate_model(model, val_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Your Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your model achieved a validation score of 100 or higher, save your model with the cell below and submit it to the submission server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model(model, \"facial_keypoints.p\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission Instructions\n",
    "1. Go on [our submission page](https://dvl.in.tum.de/teaching/submission/), register for an account and login. We use your matriculation number and send an email with the login details to the mail account associated. When in doubt, login into tum online and check your mails there. You will get an ID which we need in the next step.\n",
    "2. Navigate to `exercise_code` directory and run the `create_submission.sh` file to create the zip file of your model. This will create a single `zip` file that you need to upload. Otherwise, you can also zip it manually if you don't want to use the bash script. However, **make sure that the structure of the zip file is the same** as it would be when generated with the bash-script.\n",
    "3. Log into [our submission page](https://dvl.in.tum.de/teaching/submission/) with your account details and upload the `zip` file. Once successfully uploaded, you should be able to see the submitted file selectable on the top.\n",
    "4. Click on this file and run the submission script. You will get an email with your score as well as a message if you have surpassed the threshold.\n",
    "\n",
    "# Submission Goals\n",
    "\n",
    "- Goal: Implement and train a convolutional neural network for facial keypoint detection.\n",
    "- Passing Criteria: Reach **Score >= 100** on __our__ test dataset. The submission system will show you your score after you submit.\n",
    "\n",
    "- Submission start: __Saturday, June 27, 2020 - 12:00__\n",
    "- Submission deadline : __Friday, July 03, 2020 - 23:59__ \n",
    "- You can make **$\\infty$** submissions until the deadline. Your __best submission__ will be considered for bonus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
